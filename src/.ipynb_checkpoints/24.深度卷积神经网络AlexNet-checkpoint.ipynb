{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5678ff",
   "metadata": {},
   "source": [
    "# AlexNet\n",
    "\n",
    "## 1. 技术回顾\n",
    "\n",
    "1）支持向量机\n",
    "\n",
    "在2000年左右，最流行的机器学习的算法是核方法 (Learning with Kernels).\n",
    "\n",
    "the Support Vector Machine (SVM)\n",
    "\n",
    "- 特征提取\n",
    "- 选择核函数来计算相似性\n",
    "- 凸优化问题\n",
    "- 漂亮的定理（来自泛函）\n",
    "\n",
    "优点：对调参不敏感，闭着眼睛都可以用\n",
    "\n",
    "2）几何学\n",
    "\n",
    "- 抽取特征\n",
    "- 描述几何（例如多相机）\n",
    "- （非）凸优化\n",
    "- 漂亮定理\n",
    "- 如果假设满足，效果很好\n",
    "\n",
    "3）特征工程\n",
    "\n",
    "直接把特征放到SVM里效果非常差，所以需要做特征工程\n",
    "\n",
    "- （在计算机视觉里）特征工程是关键\n",
    "- 特征描述子：SIFT, SURF\n",
    "\n",
    "\n",
    "## 2. ImageNet (2020)\n",
    "\n",
    "- 图片：自然物体的彩色图片\n",
    "- 大小：$469 \\times 387$\n",
    "- 样本数 1.2M\n",
    "- 类数：1000\n",
    "\n",
    "因为你有了更大的数据集，所以允许用更深的神经网络，用来抽取信息\n",
    "\n",
    "## 3. AlexNet\n",
    "\n",
    "- AlexNet赢了2012年ImageNet竞赛\n",
    "- 更深更大的 LeNet\n",
    "- 主要改进：\n",
    "    - 丢弃法（因为模型大了，所以用丢弃做一点正则）\n",
    "    - ReLU（$Sigmoid \\space (LeNet) \\to ReLU$，ReLU在零点的一阶导更好，梯度更大）\n",
    "    - MaxPooling（数值比较大，梯度比较大，使得训练比较容易）\n",
    "   - 计算机视觉方法论的改变\n",
    "       - 核方法：人工特征提取 $\\to$ SVM\n",
    "       - 神经网络：通过 CNN 学习特征 $\\to$ Softmax回归\n",
    "\n",
    "## 4. AlexNet架构\n",
    "\n",
    "1）第一个卷积层和池化层的区别\n",
    "\n",
    "|AlexNet|LeNet|\n",
    "| -- | -- |\n",
    "|image ($3 \\times 224 \\times 224$)|image ($32 \\times 32$)|\n",
    "|11*11 Conv(96), stride 4|5*5 Conv(6), pad 2|\n",
    "|3*3 MaxPool, stride 2|2*2 AvgPool, stride 2|\n",
    "\n",
    "- 卷积：更大的窗口，更大的通道数，更大的步幅\n",
    "- 池化：更大的窗口且stride不变（对位置更不敏感），使用MaxPool\n",
    "\n",
    "2）接下来\n",
    "\n",
    "|AlexNet|LeNet|\n",
    "| -- | -- |\n",
    "|5*5 Conv(256), pad 2|5*5 Conv()|\n",
    "|3*3 MaxPool, stride 2|2*2 AvgPool, stride 2|\n",
    "|3*3 Conv(384), pad 1||\n",
    "|3*3 Conv(384), pad 1||\n",
    "|3*3 Conv(384), pad 1||\n",
    "|3*3 MaxPool, stride 2||\n",
    "\n",
    "- 新增了3层卷积层\n",
    "- 更多的输出通道\n",
    "\n",
    "3）全连接层\n",
    "\n",
    "|AlexNet|LeNet|\n",
    "| -- | -- |\n",
    "|Dense(4096)|Dense(120)|\n",
    "|Dense(4096)|Dense(84)|\n",
    "|Dense(1000)|Dense(10)|\n",
    "\n",
    "- 隐藏层大小：120 $\\to$ 4096\n",
    "- 输出类别：10 $\\to$ 1000\n",
    "\n",
    "## 5. 更多细节\n",
    "\n",
    "- 激活函数从 Sigmoid 变到了 ReLU（减缓梯度消失，因为梯度比较大）\n",
    "- 隐藏全连接层后加入了丢弃层\n",
    "- 数据增强（随机截取采样，调亮度/色温。神经网络会记住所有数据，数据增强能让记住数据的能力变低）\n",
    "\n",
    "## 6. 总结\n",
    "\n",
    "- AlexNet是更大更深的LeNet，10x参数个数，260x计算复杂度\n",
    "- 新加入了丢弃法，ReLU，最大池化层，和数据增强\n",
    "- AlexNet赢下了2012ImageNet竞赛后，标志着新的一轮神经网络热潮的开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b163e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0db448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4aaba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faffb43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf725b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2d156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
