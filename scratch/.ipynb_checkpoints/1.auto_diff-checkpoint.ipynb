{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eee6a20",
   "metadata": {},
   "source": [
    "# 自动微分\n",
    "\n",
    "教程地址：https://zh-v2.d2l.ai/chapter_preliminaries/autograd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf06df",
   "metadata": {},
   "source": [
    "## 1. 简单的例子\n",
    "\n",
    "### 1.1 张量 x 的梯度\n",
    "\n",
    "张量 $x$ 的梯度可以存储在 $x$ 上。\n",
    "\n",
    "要点：\n",
    "\n",
    "- `x.grad`: 取 $x$ 的梯度\n",
    "- `x.requires_grad_(True)`: 允许 tenser $x$ 存储自己的梯度\n",
    "- `x.grad.zero_()`: 将 $x$ 的梯度置零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7176a216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 初始化张量 x (tenser x)\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc2f54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(x.requires_grad_)\n",
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab8abf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(True)  # 允许 tensor x 存储梯度\n",
    "x.grad == None  # 梯度默认为 None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376ea03",
   "metadata": {},
   "source": [
    "初始化带梯度的张量，下面是两个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94a004c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1., 2., 3.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c29edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4565,  1.2696,  0.0927, -0.3001,  0.2633],\n",
       "        [-0.2927, -0.2221,  0.8616,  0.2330, -1.1366]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((2, 5), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a909d",
   "metadata": {},
   "source": [
    "### 1.2 损失函数 及 反向传播\n",
    "\n",
    "我们约定：\n",
    "\n",
    "- 将 **损失** 记为 $y$\n",
    "- 设 **损失函数** 为：$y = 2 * x \\cdot x$（注意是点乘）\n",
    "\n",
    "计算 $y$ 关于 $x$ 每个分量的梯度，步骤如下：\n",
    "\n",
    "1. 定义损失函数：`y = 2 * torch.dot(x, x)`\n",
    "2. 计算 $y$ 关于 $x$ 的梯度，即反向传播：`y.backward()`\n",
    "3. 获取更新后 $x$ 的梯度：`x.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668f97bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y  # 注意 y 在这里是标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9531d8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()  # 用反向传播自动计算 y 关于 x 每个分量的梯度\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5028d0e",
   "metadata": {},
   "source": [
    "函数 $y = 2x^{T}x$ 关于 $x$ 的梯度应为 $4x$，验证是否正确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab83cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c870a",
   "metadata": {},
   "source": [
    "## 2. 当损失为向量时\n",
    "\n",
    "梯度的“形状”：\n",
    "\n",
    "- 当损失 $y$ 为 **标量** 时，梯度是 **向量**，且与 $x$ 维度相同\n",
    "- 当损失 $y$ 为 **向量** 时，梯度是 **矩阵**\n",
    "\n",
    "注意，当损失 $y$ 为向量时。我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。也就是说：\n",
    "\n",
    "在反向传播代码里要多加一个 `sum()` 函数，写成 `y.sum().backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a1788db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.], requires_grad=True), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()  # 将张量 x 的梯度置零\n",
    "x, x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30de6b5",
   "metadata": {},
   "source": [
    "定义损失函数：$y = 2 * x \\times x$（注意是叉乘）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c70fc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  2.,  8., 18.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义损失函数\n",
    "y = 2 * x * x\n",
    "y  # 注意 y 在这里是向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bae63ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum().backward()  # 等价于 y.backward(torch.ones(len(x)))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89552f71",
   "metadata": {},
   "source": [
    "## 3. `with torch.no_grad()` \n",
    "\n",
    "在 PyTorch 中，如果一个张量的 `requires_grad` 参数设为 `True`。则所有依赖它的张量的 `requires_grad` 参数将被设置为 `True`\n",
    "\n",
    "但在 `with torch.no_grad()` 块中的张量，依赖它的张量的 `requires_grad` 参数将被设为 `False`\n",
    "\n",
    "参见：https://pytorch.org/docs/stable/generated/torch.no_grad.html\n",
    "\n",
    "下面是一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a64b4ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ec19d",
   "metadata": {},
   "source": [
    "作为对照："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "616ef43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.], requires_grad=True)\n",
    "y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439f2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
